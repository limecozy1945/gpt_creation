This project is a PyTorch-based recreation of a GPT-style transformer language model built to deeply understand autoregressive text generation and attention mechanisms from scratch. The model consists of 12 transformer layers with 12 attention heads, a hidden size of 768, and ~110M parameters, trained on a custom character-level corpus (~10K tokens) using the AdamW optimizer and cross-entropy loss. I implemented custom tokenization, batching, and multinomial sampling for next-token prediction, and reduced training loss by ~15â€“20% compared to a baseline bigram model while achieving stable convergence. The project focuses on understanding self-attention, residual connections, layer normalization, and scaling behavior rather than using pretrained weights, strengthening my foundations in NLP, deep learning, and generative AI systems design.
